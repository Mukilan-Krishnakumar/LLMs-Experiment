{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d58a6b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e0e3e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing OS and setting environment variables\n",
    "import os\n",
    "from apikeys import OPEN_AI_KEY, DEEP_LAKE_KEY, HUGGINGFACE_TOKEN\n",
    "\n",
    "# For logging wandb langchain\n",
    "os.environ[\"LANGCHAIN_WANDB_TRACING\"] = \"true\"\n",
    "\n",
    "# Setting wandb project details\n",
    "os.environ[\"WANDB_PROJECT\"] = \"langchain\"\n",
    "\n",
    "# Setting up openai environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPEN_AI_KEY\n",
    "\n",
    "# Setting up hugging face token\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42a5dee",
   "metadata": {},
   "source": [
    "# Introduction to Large Language Models\n",
    "\n",
    "## Token Distributions and Predicting the Next Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8db2569a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "One Piece Sailing Company.\n"
     ]
    }
   ],
   "source": [
    "from langchain import OpenAI\n",
    "llm = OpenAI(model_name = \"text-davinci-003\", temperature = 0)\n",
    "text = \"What is a good company name for a company that sails in the ocean and represents One Piece?\"\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0db37fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  \n",
      "\n",
      "Q: What did the fish say when it hit the wall?\n",
      "A: Dam!\n",
      "Tokens Used: 29\n",
      "\tPrompt Tokens: 9\n",
      "\tCompletion Tokens: 20\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.00058\n"
     ]
    }
   ],
   "source": [
    "# Tracking token usage\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm(\"Generate a Seinfield sort of joke\")\n",
    "    print(\"Output: \", result)\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2af5a40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "# Create the examples set\n",
    "examples = [\n",
    "        {\n",
    "        \"query\" : \"Fish\",\n",
    "        \"answer\" : \"What did the fish say when it hit the wall? Dam!\"\n",
    "        },{\n",
    "        \"query\" : \"Kendrick Lamar\",\n",
    "        \"answer\" : \"What did Kendrick Lamar say when he hit the wall? Damn!\"\n",
    "        },\n",
    "        # forgive my dumb jokes\n",
    "        {\n",
    "        \"query\" : \"Pitbull\",\n",
    "        \"answer\" : \"Why is Pitbull the world's most popular singer? Because he is Mr. Worldwide\"\n",
    "        }\n",
    "]\n",
    "\n",
    "# Creating the example template\n",
    "example_template = \"\"\"\n",
    "User : {query},\n",
    "Answer : {answer}\n",
    "\"\"\"\n",
    "\n",
    "# Creating prompt template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables = [\"query\", \"answer\"],\n",
    "    template = example_template\n",
    ")\n",
    "\n",
    "# Now we can have prefix and suffix for our input and dynamically feed in examples\n",
    "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
    "assistant. The assistant is known for telling dumb jokes. Here are some examples:\"\"\"\n",
    "\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI:\n",
    "\"\"\"\n",
    "\n",
    "# Let's create a few shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples = examples,\n",
    "    example_prompt = example_prompt,\n",
    "    prefix = prefix,\n",
    "    suffix = suffix,\n",
    "    input_variables = [\"query\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea531619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6146452",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90a60469",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=chat, prompt=few_shot_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53442a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What did J Cole say when he hit the wall? \"I guess it\\'s just another brick in the Cole.\"'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"J Cole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6b36ff",
   "metadata": {},
   "source": [
    "`wandb` logging can only take place when we run agents, not in any other case. Running a chain won't do anything. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a5d42f",
   "metadata": {},
   "source": [
    "## Question Answering Bot\n",
    "\n",
    "First, we need a template for answering questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eddba8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Question : {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template = template,\n",
    "    input_variables = [\"question\"]\n",
    ")\n",
    "\n",
    "# Now, we can enter question\n",
    "question = \"Where is Bordeaux?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9f03d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flanders\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub, LLMChain\n",
    "\n",
    "hub_llm = HuggingFaceHub(\n",
    "    repo_id = \"google/flan-t5-large\",\n",
    "    model_kwargs={'temperature':0}\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt = prompt,\n",
    "    llm = hub_llm\n",
    ")\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24222987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paris\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the capital of France?\"\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b8ad7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gerald ford\n"
     ]
    }
   ],
   "source": [
    "question = \"Who is the president of America?\"\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7354cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can put it all in a list and ask multiple questions\n",
    "question_list = [\n",
    "    {\"question\" : \"What is the capital of France?\"},\n",
    "    {\"question\" : \"Who is th president of America?\"},\n",
    "    {\"question\" : \"What is the best J Cole Album?\"},\n",
    "    {\"question\" : \"Who is the greatest rapper of all time?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b86053b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='paris', generation_info=None)], [Generation(text='abraham lincoln', generation_info=None)], [Generation(text='i want to be king', generation_info=None)], [Generation(text='dr dre', generation_info=None)]] llm_output=None\n"
     ]
    }
   ],
   "source": [
    "response = llm_chain.generate(question_list)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4b375c",
   "metadata": {},
   "source": [
    "## Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a60eabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-3.5-turbo\", temperature = 0)\n",
    "\n",
    "summarization_template = \"Summarize the following text into one sentence: {text}\"\n",
    "summarization_prompt = PromptTemplate(input_variables = [\"text\"], template = summarization_template)\n",
    "summarization_chain = LLMChain(llm = llm, prompt = summarization_prompt)\n",
    "\n",
    "\n",
    "text = \"Girard's main contribution to philosophy, and in turn to other disciplines, was in the psychology of desire. Girard claimed that human desire functions imitatively, or mimetically, rather than arising as the spontaneous byproduct of human individuality, as much of theoretical psychology had assumed. Girard found that human development proceeds triangularly from a model of desire who indicates some object of desire as desirable by desiring it themselves. We copy this desire for the object of the model and appropriate it as our own, most often without recognizing that the source of this desire comes from another apart from ourselves completing the triangle of mimetic desire. This process of appropriation of desire includes (but is not limited to) identity formation, the transmission of knowledge and social norms, and material aspirations which all have their origin in copying the desires of others who we take, consciously or unconsciously, as models for desire.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c32e0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_text = summarization_chain.predict(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "faabdf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Girard's main contribution to philosophy was the understanding that human desire is imitative, meaning that it is not a spontaneous result of individuality but rather a process of copying the desires of others, which influences identity formation, knowledge transmission, social norms, and material aspirations.\n"
     ]
    }
   ],
   "source": [
    "print(summarized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d86b1de",
   "metadata": {},
   "source": [
    "## Text Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf2195ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_template = \"Translate the following from {language1} to {language2} : {text}\"\n",
    "translation_prompt = PromptTemplate(input_variables = [\"language1\",\"language2\",\"text\"], template = translation_template)\n",
    "translation_chain = LLMChain(llm = llm, prompt = translation_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2592d039",
   "metadata": {},
   "outputs": [],
   "source": [
    "language1 = \"English\"\n",
    "language2 = \"French\"\n",
    "text = \"Life is Good, life is good my friend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49018291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n"
     ]
    }
   ],
   "source": [
    "translated_text = translation_chain.predict(language1 = language1, language2 = language2, text = text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff561ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La vie est belle, la vie est belle mon ami.\n"
     ]
    }
   ],
   "source": [
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957cc0d3",
   "metadata": {},
   "source": [
    "## Testing out Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1b551ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16848e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8038ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93d8026e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:    ['This', 'Ġis', 'Ġa', 'Ġsample', 'Ġtext', 'Ġto', 'Ġtest', 'Ġthe', 'Ġtoken', 'izer', '.']\n",
      "Token IDs: [1212, 318, 257, 6291, 2420, 284, 1332, 262, 11241, 7509, 13]\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.encode(\"This is a sample text to test the tokenizer.\")\n",
    "\n",
    "print( \"Tokens:   \", tokenizer.convert_ids_to_tokens( token_ids ) )\n",
    "print( \"Token IDs:\", token_ids )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bc432d",
   "metadata": {},
   "source": [
    "## W&B Prompt Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a69ec249",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/LangChain/lib/python3.11/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.6.15) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import load_tools, initialize_agent, AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "964a0c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)\n",
    "tools = load_tools([\"llm-math\"], llm=llm)\n",
    "math_agent = initialize_agent(tools, \n",
    "                              llm, \n",
    "                              agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac016a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Streaming LangChain activity to W&B at https://wandb.ai/mukilan/langchain/runs/qh9amx8t\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: `WandbTracer` is currently in beta.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Please report any issues to https://github.com/wandb/wandb/issues with the tag `langchain`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMMathChain._evaluate(\"\n",
      "import math\n",
      "math.sqrt(5.4)\n",
      "\") raised error: invalid syntax (<expr>, line 1). Please try again with a valid numerical expression\n",
      "0.005720801417544866\n",
      "0.15096209512635608\n"
     ]
    }
   ],
   "source": [
    "# some sample maths questions\n",
    "questions = [\n",
    "  \"Find the square root of 5.4.\",\n",
    "  \"What is 3 divided by 7.34 raised to the power of pi?\",\n",
    "  \"What is the sin of 0.47 radians, divided by the cube root of 27?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "  try:\n",
    "    # call your Agent as normal\n",
    "    answer = math_agent.run(question)\n",
    "    print(answer)\n",
    "  except Exception as e:\n",
    "    # any errors will be also logged to Weights & Biases\n",
    "    print(e)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f71e654c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_agent.run(\"What is 2 times 5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16ff5813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with open source models\n",
    "from langchain import HuggingFaceHub, LLMChain\n",
    "\n",
    "hub_llm = HuggingFaceHub(\n",
    "    repo_id = \"google/flan-t5-large\",\n",
    "    model_kwargs={'temperature':0}\n",
    ")\n",
    "\n",
    "tools = load_tools([\"llm-math\"], llm=hub_llm)\n",
    "math_agent = initialize_agent(tools, \n",
    "                              hub_llm, \n",
    "                              agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8d82a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not parse LLM output: `5.4 = 5.4 * 2 = 11.0`\n",
      "Could not parse LLM output: `3 divided by 7.34 raised to the power of pi = 3 * 7 * 34 = `\n",
      "Could not parse LLM output: `The sin of 0.47 radians, divided by the cube root of 27 is 27`\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "  \"Find the square root of 5.4.\",\n",
    "  \"What is 3 divided by 7.34 raised to the power of pi?\",\n",
    "  \"What is the sin of 0.47 radians, divided by the cube root of 27?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "  try:\n",
    "    # call your Agent as normal\n",
    "    answer = math_agent.run(question)\n",
    "    print(answer)\n",
    "  except Exception as e:\n",
    "    # any errors will be also logged to Weights & Biases\n",
    "    print(e)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92699e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model = \"gpt-3.5-turbo\", temperature = 0)\n",
    "tools = load_tools([\"llm-math\"], llm=llm)\n",
    "math_agent = initialize_agent(tools, \n",
    "                              llm, \n",
    "                              agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5507254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMMathChain._evaluate(\"\n",
      "import math\n",
      "math.sqrt(5.4)\n",
      "\") raised error: invalid syntax (<expr>, line 1). Please try again with a valid numerical expression\n",
      "0.005720801417544866\n",
      "The sin of 0.47 radians, divided by the cube root of 27, is approximately 0.15096209512635608.\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "  \"Find the square root of 5.4.\",\n",
    "  \"What is 3 divided by 7.34 raised to the power of pi?\",\n",
    "  \"What is the sin of 0.47 radians, divided by the cube root of 27?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "  try:\n",
    "    # call your Agent as normal\n",
    "    answer = math_agent.run(question)\n",
    "    print(answer)\n",
    "  except Exception as e:\n",
    "    # any errors will be also logged to Weights & Biases\n",
    "    print(e)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a98ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
