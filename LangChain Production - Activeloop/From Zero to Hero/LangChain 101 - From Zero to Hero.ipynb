{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5d07b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "922fcbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store your API keys in a common file and reference it here\n",
    "from apikeys import OPEN_AI_KEY, DEEP_LAKE_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b7fd0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = OPEN_AI_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27706d41",
   "metadata": {},
   "source": [
    "## The LLMs\n",
    "\n",
    "With LangChain, we can call a LLM on our prompt, we are going to do that right now. I am going to ask the LLM to give me a top 10 tips for efficiently managing time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5688ccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06df34d9",
   "metadata": {},
   "source": [
    "The `temperature` parameter determines the randomness, `0` is stable and `1` is random. Creative tasks should be in the range of `0.7` and `0.9`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d43aa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_davinci = OpenAI(model_name = \"text-davinci-003\", temperature= 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba26abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Give me the top 10 tips for efficiently managing time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e460611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips = llm_davinci(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c75205a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Prioritize your tasks â€“ understand what is most important to get done and focus on completing those things first.\n",
      "\n",
      "2. Create a plan â€“ map out each day, week, and month so you know what needs to be accomplished and when.\n",
      "\n",
      "3. Eliminate distractions â€“ turn off notifications and focus on one task at a time to stay on track.\n",
      "\n",
      "4. Tackle the difficult tasks first â€“ doing the hard tasks when you have the most energy will help you stay motivated throughout the day.\n",
      "\n",
      "5. Break down large projects â€“ breaking down larger projects into smaller chunks makes them more manageable and allows for better progress tracking.\n",
      "\n",
      "6. Utilize task management tools â€“ there are many tools available to help manage tasks and keep track of progress.\n",
      "\n",
      "7. Take frequent breaks â€“ working non-stop is not effective â€“ taking short breaks throughout the day can help you stay focused and productive.\n",
      "\n",
      "8. Set realistic goals â€“ setting achievable goals helps you stay motivated and focused on the tasks at hand.\n",
      "\n",
      "9. Outsource when possible â€“ donâ€™t try to do everything yourself, outsource certain tasks or ask for help when you need it.\n",
      "\n",
      "10. Be flexible â€“ life throws curveballs\n"
     ]
    }
   ],
   "source": [
    "print(tips)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b65e3dc",
   "metadata": {},
   "source": [
    "## The Chains\n",
    "Chains are end-to-end wrappers around components, used to create sequences. The most commonly used chain is `LLMChain`. \n",
    "\n",
    "`PromptTemplate` removes the overhead of repeating sentences. In any use case, we can abstract the idea into modules, take the input and provide the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22ddc71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52f513fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAI(model_name = \"text-davinci-003\", temperature = 0.9)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables= [\"number\", \"product\"],\n",
    "    template = \"Give me the top {number} places to buy {product}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65ef9aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm = model, prompt= prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "736a37c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='Give me the top 5 places to buy chocolate'\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format_prompt(number = \"5\", product = \"chocolate\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2d1be56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Lindt Chocolates\n",
      "2. Godiva Chocolates\n",
      "3. Ghirardelli Chocolates\n",
      "4. Hershey's Chocolate World\n",
      "5. Russell Stover Chocolates\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(number = \"5\", product = \"chocolate\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4d242d",
   "metadata": {},
   "source": [
    "## The Memory\n",
    "Memory as the name suggest is the mechanism for storing and managing conversation history, to enable the AI to generate more relevant responses. \n",
    "\n",
    "One such method is `ConversationalBufferMemory`, it wraps around `ChatMessageHistory`, extracts messages and gives them to chain. It helps in better context-aware generation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16696609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f72ab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAI(model_name = \"text-davinci-003\", temperature = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5383ecb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm = model,\n",
    "    verbose = True,\n",
    "    memory = ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f23798e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hey, how are you?\n",
      "AI:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi there! I'm doing great! It has been a productive day with lots of exciting new experiences. How about you?\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starting the conversation\n",
    "conversation.predict(input = \"Hey, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9be37d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hey, how are you?\n",
      "AI:  Hi there! I'm doing great! It has been a productive day with lots of exciting new experiences. How about you?\n",
      "Human: What can you help me with?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hey, how are you?\n",
      "AI:  Hi there! I'm doing great! It has been a productive day with lots of exciting new experiences. How about you?\n",
      "Human: What can you help me with?\n",
      "AI:  Glad you asked! I can help you with a wide range of tasks. What specifically do you need assistance with?\n",
      "Human: Appreciate it\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "memory=ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Hey, how are you?', additional_kwargs={}, example=False), AIMessage(content=\" Hi there! I'm doing great! It has been a productive day with lots of exciting new experiences. How about you?\", additional_kwargs={}, example=False), HumanMessage(content='What can you help me with?', additional_kwargs={}, example=False), AIMessage(content=' Glad you asked! I can help you with a wide range of tasks. What specifically do you need assistance with?', additional_kwargs={}, example=False), HumanMessage(content='Appreciate it', additional_kwargs={}, example=False), AIMessage(content=\" You're welcome! Is there anything else I can do for you?\", additional_kwargs={}, example=False)]), output_key=None, input_key=None, return_messages=False, human_prefix='Human', ai_prefix='AI', memory_key='history') callbacks=None callback_manager=None verbose=True prompt=PromptTemplate(input_variables=['history', 'input'], output_parser=None, partial_variables={}, template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:', template_format='f-string', validate_template=True) llm=OpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.9, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key=None, openai_api_base=None, openai_organization=None, batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all') output_key='response' input_key='input'\n"
     ]
    }
   ],
   "source": [
    "# Continuing the conversation \n",
    "conversation.predict(input = \"What can you help me with?\")\n",
    "conversation.predict(input = \"Appreciate it\")\n",
    "\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1c5506",
   "metadata": {},
   "source": [
    "Memory is basically log here. What about Context Window? When does the model forget? Questions we will be exploring. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce82b58",
   "metadata": {},
   "source": [
    "## Deep Lake VectorStore\n",
    "As the name suggests, it is a vector store. It stores vector embeddings. This can be conversation, information or knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fc3bf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"ACTIVELOOP_TOKEN\"] = DEEP_LAKE_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9168b0",
   "metadata": {},
   "source": [
    "Install the deeplake library - `pip install deeplake`. I have run this command but the output is too long, so I am gonna cut that cell out. \n",
    "\n",
    "> ðŸ§  Do remember to `!pip install deeplake`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06095685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import DeepLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6563aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model which is not at all random, very reproducable\n",
    "model = OpenAI(model_name = \"text-davinci-003\", temperature = 0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f43292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our documents - databases \n",
    "texts = [\n",
    "    \"Napoleon was defated in the Battle of Waterloo\",\n",
    "    \"Battle of Austerlitz is considered the greatest victory of Napoleon Bonaparte\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13554bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 0)\n",
    "docs = text_splitter.create_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6d1ffd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Napoleon was defated in the Battle of Waterloo', metadata={}), Document(page_content='Battle of Austerlitz is considered the greatest victory of Napoleon Bonaparte', metadata={})]\n"
     ]
    }
   ],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12ba8a1",
   "metadata": {},
   "source": [
    "Recursive Character Text Splitter - Recursively Splits texts (First paragraph, then sentences, then words), the idea is to maintain the connectedness as long as possible introducing semantic meaning. \n",
    "\n",
    "We can now create a DeepLake dataset and upload the docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51a03e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_activeloop_org_id = \"mukilan\"\n",
    "my_activeloop_dataset_name = \"langchain_course_from_zero_to_hero\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0c46ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/mukilan/langchain_course_from_zero_to_hero\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://mukilan/langchain_course_from_zero_to_hero loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    }
   ],
   "source": [
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38b7c618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating ingest: 0%|                                            | 0/1 [00:00<?Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n",
      "Evaluating ingest: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00\n",
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://mukilan/langchain_course_from_zero_to_hero', tensors=['embedding', 'ids', 'metadata', 'text'])\n",
      "\n",
      "  tensor     htype     shape     dtype  compression\n",
      "  -------   -------   -------   -------  ------- \n",
      " embedding  generic  (2, 1536)  float32   None   \n",
      "    ids      text     (2, 1)      str     None   \n",
      " metadata    json     (2, 1)      str     None   \n",
      "   text      text     (2, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['91a8e1c0-2b44-11ee-8634-5e6895b2d216',\n",
       " '91a8e314-2b44-11ee-8634-5e6895b2d216']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can finally add documents to the DeepLake\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d252f0b",
   "metadata": {},
   "source": [
    "Moving on to creating a RetrievalQA Chain which will be used as tool for the upcoming agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ed52c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_qa = RetrievalQA.from_chain_type(\n",
    "    llm = model,\n",
    "    chain_type= \"stuff\",\n",
    "    retriever = db.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019ff169",
   "metadata": {},
   "source": [
    "Here, the `stuff` chain type is used. It is typically used for small documents and only few are passed in at most calls. It takes a list of documents, inserts them into prompts and sends them to LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1370a7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool, AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98ebfec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name = \"Retrieval QA system\",\n",
    "        func= retrieval_qa.run,\n",
    "        description= \"Useful for answering questions\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    model,\n",
    "    agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b70960b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m I should research Napoleon's battles\n",
      "Action: Retrieval QA system\n",
      "Action Input: Napoleon's greatest victory\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m The Battle of Austerlitz\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The Battle of Austerlitz is considered the greatest victory for Napoleon.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = agent.run(\"Which battle is considered the greatest victory for Napoleon?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ba2e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
