{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5d07b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "922fcbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store your API keys in a common file and reference it here\n",
    "from apikeys import OPEN_AI_KEY, DEEP_LAKE_KEY, GOOGLE_API_KEY, GOOGLE_CSE_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b7fd0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = OPEN_AI_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27706d41",
   "metadata": {},
   "source": [
    "## The LLMs\n",
    "\n",
    "With LangChain, we can call a LLM on our prompt, we are going to do that right now. I am going to ask the LLM to give me a top 10 tips for efficiently managing time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5688ccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06df34d9",
   "metadata": {},
   "source": [
    "The `temperature` parameter determines the randomness, `0` is stable and `1` is random. Creative tasks should be in the range of `0.7` and `0.9`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d43aa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_davinci = OpenAI(model_name = \"text-davinci-003\", temperature= 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba26abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Give me the top 10 tips for efficiently managing time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e460611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips = llm_davinci(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c75205a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Prioritize your tasks â€“ understand what is most important to get done and focus on completing those things first.\n",
      "\n",
      "2. Create a plan â€“ map out each day, week, and month so you know what needs to be accomplished and when.\n",
      "\n",
      "3. Eliminate distractions â€“ turn off notifications and focus on one task at a time to stay on track.\n",
      "\n",
      "4. Tackle the difficult tasks first â€“ doing the hard tasks when you have the most energy will help you stay motivated throughout the day.\n",
      "\n",
      "5. Break down large projects â€“ breaking down larger projects into smaller chunks makes them more manageable and allows for better progress tracking.\n",
      "\n",
      "6. Utilize task management tools â€“ there are many tools available to help manage tasks and keep track of progress.\n",
      "\n",
      "7. Take frequent breaks â€“ working non-stop is not effective â€“ taking short breaks throughout the day can help you stay focused and productive.\n",
      "\n",
      "8. Set realistic goals â€“ setting achievable goals helps you stay motivated and focused on the tasks at hand.\n",
      "\n",
      "9. Outsource when possible â€“ donâ€™t try to do everything yourself, outsource certain tasks or ask for help when you need it.\n",
      "\n",
      "10. Be flexible â€“ life throws curveballs\n"
     ]
    }
   ],
   "source": [
    "print(tips)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f16b89",
   "metadata": {},
   "source": [
    "## The Chains\n",
    "Chains are end-to-end wrappers around components, used to create sequences. The most commonly used chain is `LLMChain`. \n",
    "\n",
    "`PromptTemplate` removes the overhead of repeating sentences. In any use case, we can abstract the idea into modules, take the input and provide the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bd9d051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2465b27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAI(model_name = \"text-davinci-003\", temperature = 0.9)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables= [\"number\", \"product\"],\n",
    "    template = \"Give me the top {number} places to buy {product}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5208ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm = model, prompt= prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "016fd643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='Give me the top 5 places to buy chocolate'\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format_prompt(number = \"5\", product = \"chocolate\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76751e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Lindt Chocolates\n",
      "2. Godiva Chocolates\n",
      "3. Ghirardelli Chocolates\n",
      "4. Hershey's Chocolate World\n",
      "5. Russell Stover Chocolates\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(number = \"5\", product = \"chocolate\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feb292d",
   "metadata": {},
   "source": [
    "## The Memory\n",
    "Memory as the name suggest is the mechanism for storing and managing conversation history, to enable the AI to generate more relevant responses. \n",
    "\n",
    "One such method is `ConversationalBufferMemory`, it wraps around `ChatMessageHistory`, extracts messages and gives them to chain. It helps in better context-aware generation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c377073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58bd19c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAI(model_name = \"text-davinci-003\", temperature = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9854f705",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm = model,\n",
    "    verbose = True,\n",
    "    memory = ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9bf372f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hey, how are you?\n",
      "AI:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi there! I'm doing great! It has been a productive day with lots of exciting new experiences. How about you?\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starting the conversation\n",
    "conversation.predict(input = \"Hey, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64120446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hey, how are you?\n",
      "AI:  Hi there! I'm doing great! It has been a productive day with lots of exciting new experiences. How about you?\n",
      "Human: What can you help me with?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hey, how are you?\n",
      "AI:  Hi there! I'm doing great! It has been a productive day with lots of exciting new experiences. How about you?\n",
      "Human: What can you help me with?\n",
      "AI:  Glad you asked! I can help you with a wide range of tasks. What specifically do you need assistance with?\n",
      "Human: Appreciate it\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "memory=ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Hey, how are you?', additional_kwargs={}, example=False), AIMessage(content=\" Hi there! I'm doing great! It has been a productive day with lots of exciting new experiences. How about you?\", additional_kwargs={}, example=False), HumanMessage(content='What can you help me with?', additional_kwargs={}, example=False), AIMessage(content=' Glad you asked! I can help you with a wide range of tasks. What specifically do you need assistance with?', additional_kwargs={}, example=False), HumanMessage(content='Appreciate it', additional_kwargs={}, example=False), AIMessage(content=\" You're welcome! Is there anything else I can do for you?\", additional_kwargs={}, example=False)]), output_key=None, input_key=None, return_messages=False, human_prefix='Human', ai_prefix='AI', memory_key='history') callbacks=None callback_manager=None verbose=True prompt=PromptTemplate(input_variables=['history', 'input'], output_parser=None, partial_variables={}, template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:', template_format='f-string', validate_template=True) llm=OpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.9, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key=None, openai_api_base=None, openai_organization=None, batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all') output_key='response' input_key='input'\n"
     ]
    }
   ],
   "source": [
    "# Continuing the conversation \n",
    "conversation.predict(input = \"What can you help me with?\")\n",
    "conversation.predict(input = \"Appreciate it\")\n",
    "\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67b7c72",
   "metadata": {},
   "source": [
    "Memory is basically log here. What about Context Window? When does the model forget? Questions we will be exploring. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ef134a",
   "metadata": {},
   "source": [
    "## Deep Lake VectorStore\n",
    "As the name suggests, it is a vector store. It stores vector embeddings. This can be conversation, information or knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e287e2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"ACTIVELOOP_TOKEN\"] = DEEP_LAKE_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cdc4c9",
   "metadata": {},
   "source": [
    "Install the deeplake library - `pip install deeplake`. I have run this command but the output is too long, so I am gonna cut that cell out. \n",
    "\n",
    "> ğŸ§  Do remember to `!pip install deeplake`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2e04013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import DeepLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff0244c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model which is not at all random, very reproducable\n",
    "model = OpenAI(model_name = \"text-davinci-003\", temperature = 0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8be4ad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our documents - databases \n",
    "texts = [\n",
    "    \"Napoleon was defated in the Battle of Waterloo\",\n",
    "    \"Battle of Austerlitz is considered the greatest victory of Napoleon Bonaparte\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23e4baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 0)\n",
    "docs = text_splitter.create_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7d39a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Napoleon was defated in the Battle of Waterloo', metadata={}), Document(page_content='Battle of Austerlitz is considered the greatest victory of Napoleon Bonaparte', metadata={})]\n"
     ]
    }
   ],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19bf0bf",
   "metadata": {},
   "source": [
    "Recursive Character Text Splitter - Recursively Splits texts (First paragraph, then sentences, then words), the idea is to maintain the connectedness as long as possible introducing semantic meaning. \n",
    "\n",
    "We can now create a DeepLake dataset and upload the docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f9c9eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_activeloop_org_id = \"mukilan\"\n",
    "my_activeloop_dataset_name = \"langchain_course_from_zero_to_hero\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51b0759d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/mukilan/langchain_course_from_zero_to_hero\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://mukilan/langchain_course_from_zero_to_hero loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    }
   ],
   "source": [
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86ab0ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating ingest: 0%|                                            | 0/1 [00:00<?Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n",
      "Evaluating ingest: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00\n",
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://mukilan/langchain_course_from_zero_to_hero', tensors=['embedding', 'ids', 'metadata', 'text'])\n",
      "\n",
      "  tensor     htype     shape     dtype  compression\n",
      "  -------   -------   -------   -------  ------- \n",
      " embedding  generic  (2, 1536)  float32   None   \n",
      "    ids      text     (2, 1)      str     None   \n",
      " metadata    json     (2, 1)      str     None   \n",
      "   text      text     (2, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['91a8e1c0-2b44-11ee-8634-5e6895b2d216',\n",
       " '91a8e314-2b44-11ee-8634-5e6895b2d216']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can finally add documents to the DeepLake\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d113900",
   "metadata": {},
   "source": [
    "Moving on to creating a RetrievalQA Chain which will be used as tool for the upcoming agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9656ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_qa = RetrievalQA.from_chain_type(\n",
    "    llm = model,\n",
    "    chain_type= \"stuff\",\n",
    "    retriever = db.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091adce9",
   "metadata": {},
   "source": [
    "Here, the `stuff` chain type is used. It is typically used for small documents and only few are passed in at most calls. It takes a list of documents, inserts them into prompts and sends them to LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01a44d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool, AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4eb4193",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name = \"Retrieval QA system\",\n",
    "        func= retrieval_qa.run,\n",
    "        description= \"Useful for answering questions\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    model,\n",
    "    agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "656f1f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m I should research Napoleon's battles\n",
      "Action: Retrieval QA system\n",
      "Action Input: Napoleon's greatest victory\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m The Battle of Austerlitz\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The Battle of Austerlitz is considered the greatest victory for Napoleon.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = agent.run(\"Which battle is considered the greatest victory for Napoleon?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "73d82720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Battle of Austerlitz is considered the greatest victory for Napoleon.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdc60dc",
   "metadata": {},
   "source": [
    "## Agents \n",
    "Agents are components which use LLMs to determine the suitable action to take along with the order. \n",
    "\n",
    "> Action : Using a tool, observing output or returning it to user\n",
    "\n",
    "> Tool : Functions that perform specific duties such as Google Search, Database Lookups.\n",
    "\n",
    "There are many types of agents:\n",
    "1. `zero-shot-description` - Uses `ReAct` framework to decide best tool based on tool's description\n",
    "2. `react-docstore` - Goes to docstore and does two things: Search (Search tool) and Lookup (Lookup tool)\n",
    "3. `self-ask-with-search` - Intermediate Answer tool which is capable of looking up factual answers to queries\n",
    "4. `conversational-react-description` - Remembers past conversations and responds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb0b8a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.agents import AgentType, load_tools, initialize_agent, Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d140dddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAI(model_name = \"text-davinci-003\", temperature = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afa3263a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1e47c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_CSE_ID\"] = GOOGLE_CSE_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cce9ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-api-python-client\n",
      "  Downloading google_api_python_client-2.95.0-py2.py3-none-any.whl (11.5 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting httplib2<1.dev0,>=0.15.0\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m96.9/96.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth<3.0.0.dev0,>=1.19.0\n",
      "  Downloading google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m181.8/181.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-httplib2>=0.1.0\n",
      "  Using cached google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5\n",
      "  Downloading google_api_core-2.11.1-py3-none-any.whl (120 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m120.5/120.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting uritemplate<5,>=3.0.1\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.59.1-py2.py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/anaconda3/envs/LangChain/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (4.23.4)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/anaconda3/envs/LangChain/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.29.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/anaconda3/envs/LangChain/lib/python3.11/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (5.3.1)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/anaconda3/envs/LangChain/lib/python3.11/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (1.16.0)\n",
      "Requirement already satisfied: urllib3<2.0 in /usr/local/anaconda3/envs/LangChain/lib/python3.11/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (1.26.15)\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2\n",
      "  Downloading pyparsing-3.1.0-py3-none-any.whl (102 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m102.6/102.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/anaconda3/envs/LangChain/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/anaconda3/envs/LangChain/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/envs/LangChain/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2022.12.7)\n",
      "Installing collected packages: uritemplate, pyparsing, pyasn1, googleapis-common-protos, rsa, pyasn1-modules, httplib2, google-auth, google-auth-httplib2, google-api-core, google-api-python-client\n",
      "Successfully installed google-api-core-2.11.1 google-api-python-client-2.95.0 google-auth-2.22.0 google-auth-httplib2-0.1.0 googleapis-common-protos-1.59.1 httplib2-0.22.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 pyparsing-3.1.0 rsa-4.9 uritemplate-4.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e9fc968",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GoogleSearchAPIWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8f39e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name = \"google-search\",\n",
    "        func = search.run,\n",
    "        description= \"Useful for searching google to answer current events\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f4b63fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(\n",
    "    tools, \n",
    "    model,\n",
    "    agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose = True,\n",
    "    max_iterations = 6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40fc5a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m I should look for recent news articles about Twitter\n",
      "Action: google-search\n",
      "Action Input: \"Twitter news\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m@BBCWorld. News, features and analysis from the World's newsroom. Breaking news, follow. @BBCBreaking . UK news,. @BBCNews . Latest sports news. 1 day ago ... Twitter has officially rebranded to \"X\" after owner Elon Musk changed ... head of ad sales for NBCUniversal, the parent company of NBC News. BREAKING: JOE BIDEN WINS Joe Biden will be the 46th president of the United States, CNN projects, after a victory in Pennsylvania puts the Scranton-bornÂ ... #WorldCup on Twitter: The G.O.A.T. ... Product Twitter Blue is back. And gold checkmarks are here! ... X Corp. About the company Â· Twitter for Good Â· Company newsÂ ... NEW NEWS! We've got a Kelce Bowl. #SBLVII. Image. New Heights and 2 others. 1,546. 10.7K. 64.3K. 9.1M Â· Square profile picture. 9 hours ago ... Elon Musk, who has long had a fascination with the letter X, is taking another step in turning the social platform into an \"everything app.\" Giant news from a giant planet! @NASAWebb. captured a new view of Jupiter in infrared light, uncovering clues to the planet's inner life. Sep 12, 2022 ... Whether it's politics, world & local news, entertainment, sports, technology, or health & wellness, Twitter has become home to some of the mostÂ ... Your Twitter account must be verified by subscribing to either Twitter Blue or ... Sign up to enjoy Twitter Advertising. ... news from blog.twitter.comÂ ... 2 days ago ... Elon Musk has unveiled a new â€œXâ€ logo to replace Twitter's famous blue bird as he follows through with a major rebranding of the socialÂ ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The latest news about Twitter includes the rebranding of the social platform to \"X\" by owner Elon Musk, the launch of Twitter Blue and Gold Checkmarks, and the capture of a new view of Jupiter in infrared light by NASAWebb.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = agent(\"What is the latest news about Twitter?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f4009df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'What is the latest news about Twitter?', 'output': 'The latest news about Twitter includes the rebranding of the social platform to \"X\" by owner Elon Musk, the launch of Twitter Blue and Gold Checkmarks, and the capture of a new view of Jupiter in infrared light by NASAWebb.'}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc92c90c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
