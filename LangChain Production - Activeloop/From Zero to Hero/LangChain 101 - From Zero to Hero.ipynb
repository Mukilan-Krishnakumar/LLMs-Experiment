{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5d07b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "922fcbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store your API keys in a common file and reference it here\n",
    "from apikeys import OPEN_AI_KEY, DEEP_LAKE_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b7fd0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = OPEN_AI_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27706d41",
   "metadata": {},
   "source": [
    "## The LLMs\n",
    "\n",
    "With LangChain, we can call a LLM on our prompt, we are going to do that right now. I am going to ask the LLM to give me a top 10 tips for efficiently managing time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5688ccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06df34d9",
   "metadata": {},
   "source": [
    "The `temperature` parameter determines the randomness, `0` is stable and `1` is random. Creative tasks should be in the range of `0.7` and `0.9`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d43aa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_davinci = OpenAI(model_name = \"text-davinci-003\", temperature= 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba26abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Give me the top 10 tips for efficiently managing time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e460611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips = llm_davinci(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c75205a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Prioritize your tasks – understand what is most important to get done and focus on completing those things first.\n",
      "\n",
      "2. Create a plan – map out each day, week, and month so you know what needs to be accomplished and when.\n",
      "\n",
      "3. Eliminate distractions – turn off notifications and focus on one task at a time to stay on track.\n",
      "\n",
      "4. Tackle the difficult tasks first – doing the hard tasks when you have the most energy will help you stay motivated throughout the day.\n",
      "\n",
      "5. Break down large projects – breaking down larger projects into smaller chunks makes them more manageable and allows for better progress tracking.\n",
      "\n",
      "6. Utilize task management tools – there are many tools available to help manage tasks and keep track of progress.\n",
      "\n",
      "7. Take frequent breaks – working non-stop is not effective – taking short breaks throughout the day can help you stay focused and productive.\n",
      "\n",
      "8. Set realistic goals – setting achievable goals helps you stay motivated and focused on the tasks at hand.\n",
      "\n",
      "9. Outsource when possible – don’t try to do everything yourself, outsource certain tasks or ask for help when you need it.\n",
      "\n",
      "10. Be flexible – life throws curveballs\n"
     ]
    }
   ],
   "source": [
    "print(tips)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249214d5",
   "metadata": {},
   "source": [
    "## The Chains\n",
    "Chains are end-to-end wrappers around components, used to create sequences. The most commonly used chain is `LLMChain`. \n",
    "\n",
    "`PromptTemplate` removes the overhead of repeating sentences. In any use case, we can abstract the idea into modules, take the input and provide the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08d101f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "accc9f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAI(model_name = \"text-davinci-003\", temperature = 0.9)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables= [\"number\", \"product\"],\n",
    "    template = \"Give me the top {number} places to buy {product}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97c8ce24",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm = model, prompt= prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8f60864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='Give me the top 5 places to buy chocolate'\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format_prompt(number = \"5\", product = \"chocolate\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97ae78e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Lindt Chocolates\n",
      "2. Godiva Chocolates\n",
      "3. Ghirardelli Chocolates\n",
      "4. Hershey's Chocolate World\n",
      "5. Russell Stover Chocolates\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(number = \"5\", product = \"chocolate\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015f427b",
   "metadata": {},
   "source": [
    "## The Memory\n",
    "Memory as the name suggest is the mechanism for storing and managing conversation history, to enable the AI to generate more relevant responses. \n",
    "\n",
    "One such method is `ConversationalBufferMemory`, it wraps around `ChatMessageHistory`, extracts messages and gives them to chain. It helps in better context-aware generation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85bc58f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78e9631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAI(model_name = \"text-davinci-003\", temperature = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a14d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm = model,\n",
    "    verbose = True,\n",
    "    memory = ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "085a737a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hey, how are you?\n",
      "AI:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi there! I'm doing great! It has been a productive day with lots of exciting new experiences. How about you?\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starting the conversation\n",
    "conversation.predict(input = \"Hey, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb1acd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hey, how are you?\n",
      "AI:  Hi there! I'm doing great! It has been a productive day with lots of exciting new experiences. How about you?\n",
      "Human: What can you help me with?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hey, how are you?\n",
      "AI:  Hi there! I'm doing great! It has been a productive day with lots of exciting new experiences. How about you?\n",
      "Human: What can you help me with?\n",
      "AI:  Glad you asked! I can help you with a wide range of tasks. What specifically do you need assistance with?\n",
      "Human: Appreciate it\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "memory=ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Hey, how are you?', additional_kwargs={}, example=False), AIMessage(content=\" Hi there! I'm doing great! It has been a productive day with lots of exciting new experiences. How about you?\", additional_kwargs={}, example=False), HumanMessage(content='What can you help me with?', additional_kwargs={}, example=False), AIMessage(content=' Glad you asked! I can help you with a wide range of tasks. What specifically do you need assistance with?', additional_kwargs={}, example=False), HumanMessage(content='Appreciate it', additional_kwargs={}, example=False), AIMessage(content=\" You're welcome! Is there anything else I can do for you?\", additional_kwargs={}, example=False)]), output_key=None, input_key=None, return_messages=False, human_prefix='Human', ai_prefix='AI', memory_key='history') callbacks=None callback_manager=None verbose=True prompt=PromptTemplate(input_variables=['history', 'input'], output_parser=None, partial_variables={}, template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:', template_format='f-string', validate_template=True) llm=OpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.9, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key=None, openai_api_base=None, openai_organization=None, batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all') output_key='response' input_key='input'\n"
     ]
    }
   ],
   "source": [
    "# Continuing the conversation \n",
    "conversation.predict(input = \"What can you help me with?\")\n",
    "conversation.predict(input = \"Appreciate it\")\n",
    "\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f047c3",
   "metadata": {},
   "source": [
    "Memory is basically log here. What about Context Window? When does the model forget? Questions we will be exploring. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cef81e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
